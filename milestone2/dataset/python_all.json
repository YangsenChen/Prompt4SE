{"0": "def GetAnnotationObjects(self) -> list:\n        \"\"\"\n        Call IUIAutomationSelectionPattern::GetCurrentAnnotationObjects.\n        Return list, a list of `Control` subclasses representing the annotations associated with this spreadsheet cell.\n        Refer https://docs.microsoft.com/en-us/windows/desktop/api/uiautomationclient/nf-uiautomationclient-iuiautomationspreadsheetitempattern-getcurrentannotationobjects\n        \"\"\"\n        eleArray = self.pattern.GetCurrentAnnotationObjects()\n        if eleArray:\n            controls = []\n            for i in range(eleArray.Length):\n                ele = eleArray.GetElement(i)\n                con = Control.CreateControlFromElement(element=ele)\n                if con:\n                    controls.append(con)\n            return controls\n        return []", "1": "def _get_tmaster_processes(self):\n    ''' get the command to start the tmaster processes '''\n    retval = {}\n    tmaster_cmd_lst = [\n        self.tmaster_binary,\n        '--topology_name=%s' % self.topology_name,\n        '--topology_id=%s' % self.topology_id,\n        '--zkhostportlist=%s' % self.state_manager_connection,\n        '--zkroot=%s' % self.state_manager_root,\n        '--myhost=%s' % self.master_host,\n        '--master_port=%s' % str(self.master_port),\n        '--controller_port=%s' % str(self.tmaster_controller_port),\n        '--stats_port=%s' % str(self.tmaster_stats_port),\n        '--config_file=%s' % self.heron_internals_config_file,\n        '--override_config_file=%s' % self.override_config_file,\n        '--metrics_sinks_yaml=%s' % self.metrics_sinks_config_file,\n        '--metricsmgr_port=%s' % str(self.metrics_manager_port),\n        '--ckptmgr_port=%s' % str(self.checkpoint_manager_port)]\n\n    tmaster_env = self.shell_env.copy() if self.shell_env is not None else {}\n    tmaster_cmd = Command(tmaster_cmd_lst, tmaster_env)\n    if os.environ.get('ENABLE_HEAPCHECK') is not None:\n      tmaster_cmd.env.update({\n          'LD_PRELOAD': \"/usr/lib/libtcmalloc.so\",\n          'HEAPCHECK': \"normal\"\n      })\n\n    retval[\"heron-tmaster\"] = tmaster_cmd\n\n    if self.metricscache_manager_mode.lower() != \"disabled\":\n      retval[\"heron-metricscache\"] = self._get_metrics_cache_cmd()\n\n    if self.health_manager_mode.lower() != \"disabled\":\n      retval[\"heron-healthmgr\"] = self._get_healthmgr_cmd()\n\n    retval[self.metricsmgr_ids[0]] = self._get_metricsmgr_cmd(\n        self.metricsmgr_ids[0],\n        self.metrics_sinks_config_file,\n        self.metrics_manager_port)\n\n    if self.is_stateful_topology:\n      retval.update(self._get_ckptmgr_process())\n\n    return retval", "2": "def get(self, cluster, environ, topology, instance):\n    '''\n    :param cluster:\n    :param environ:\n    :param topology:\n    :param instance:\n    :return:\n    '''\n    pplan = yield access.get_physical_plan(cluster, environ, topology)\n    host = pplan['stmgrs'][pplan['instances'][instance]['stmgrId']]['host']\n    result = json.loads((yield access.get_instance_pid(\n        cluster, environ, topology, instance)))\n    self.write('<pre><br/>$%s>: %s<br/><br/>%s</pre>' % (\n        host,\n        tornado.escape.xhtml_escape(result['command']),\n        tornado.escape.xhtml_escape(result['stdout'])))", "3": "def register_metric(self, name, metric, time_bucket_in_sec):\n    \"\"\"Registers a new metric to this context\"\"\"\n    collector = self.get_metrics_collector()\n    collector.register_metric(name, metric, time_bucket_in_sec)", "4": "def wait_for_current_tasks(self):\n        \"\"\"Waits for all tasks in the task list to be completed, by waiting for their\n        AppFuture to be completed. This method will not necessarily wait for any tasks\n        added after cleanup has started (such as data stageout?)\n        \"\"\"\n\n        logger.info(\"Waiting for all remaining tasks to complete\")\n        for task_id in self.tasks:\n            # .exception() is a less exception throwing way of\n            # waiting for completion than .result()\n            fut = self.tasks[task_id]['app_fu']\n            if not fut.done():\n                logger.debug(\"Waiting for task {} to complete\".format(task_id))\n                fut.exception()\n        logger.info(\"All remaining tasks completed\")", "5": "def playstyles(year=2019, timeout=timeout):\n    \"\"\"Return all playstyles in dict {id0: playstyle0, id1: playstyle1}.\n\n    :params year: Year.\n    \"\"\"\n    rc = requests.get(messages_url, timeout=timeout)\n    rc.encoding = 'utf-8'  # guessing takes huge amount of cpu time\n    rc = rc.text\n    data = re.findall('\"playstyles.%s.playstyle([0-9]+)\": \"(.+)\"' % year, rc)\n    playstyles = {}\n    for i in data:\n        playstyles[int(i[0])] = i[1]\n    return playstyles", "6": "def set_fee_asset(self, fee_asset):\n        \"\"\" Set asset to fee\n        \"\"\"\n        if isinstance(fee_asset, self.amount_class):\n            self.fee_asset_id = fee_asset[\"id\"]\n        elif isinstance(fee_asset, self.asset_class):\n            self.fee_asset_id = fee_asset[\"id\"]\n        elif fee_asset:\n            self.fee_asset_id = fee_asset\n        else:\n            self.fee_asset_id = \"1.3.0\"", "7": "def _build_admin_context(request, customer):\n        \"\"\"\n        Build common admin context.\n        \"\"\"\n        opts = customer._meta\n        codename = get_permission_codename('change', opts)\n        has_change_permission = request.user.has_perm('%s.%s' % (opts.app_label, codename))\n        return {\n            'has_change_permission': has_change_permission,\n            'opts': opts\n        }", "8": "def cleanup(self, output):\n        \"\"\"\n        Generates consistent OpenWRT/LEDE UCI output\n        \"\"\"\n        # correct indentation\n        output = output.replace('    ', '')\\\n                       .replace('\\noption', '\\n\\toption')\\\n                       .replace('\\nlist', '\\n\\tlist')\n        # convert True to 1 and False to 0\n        output = output.replace('True', '1')\\\n                       .replace('False', '0')\n        # max 2 consecutive \\n delimiters\n        output = output.replace('\\n\\n\\n', '\\n\\n')\n        # if output is present\n        # ensure it always ends with 1 new line\n        if output.endswith('\\n\\n'):\n            return output[0:-1]\n        return output", "9": "def deserialize_organization(organization_dict):\n    \"\"\"\n    Organization dict-to-object serialization\n    \"\"\"\n    return models.Organization(\n        id=organization_dict.get('id'),\n        name=organization_dict.get('name', ''),\n        short_name=organization_dict.get('short_name', ''),\n        description=organization_dict.get('description', ''),\n        logo=organization_dict.get('logo', '')\n    )", "10": "def image_plane_pix_grid_from_regular_grid(self, regular_grid):\n        \"\"\"Calculate the image-plane pixelization from a regular-grid of coordinates (and its mask).\n\n        See *grid_stacks.SparseToRegularGrid* for details on how this grid is calculated.\n\n        Parameters\n        -----------\n        regular_grid : grids.RegularGrid\n            The grid of (y,x) arc-second coordinates at the centre of every image value (e.g. image-pixels).\n        \"\"\"\n\n        pixel_scale = regular_grid.mask.pixel_scale\n\n        pixel_scales = ((regular_grid.masked_shape_arcsec[0] + pixel_scale) / (self.shape[0]),\n                        (regular_grid.masked_shape_arcsec[1] + pixel_scale) / (self.shape[1]))\n\n        return grids.SparseToRegularGrid(unmasked_sparse_grid_shape=self.shape, pixel_scales=pixel_scales,\n                                         regular_grid=regular_grid, origin=regular_grid.mask.centre)", "11": "def tfa_templates_lclist(\n        lclist,\n        lcinfo_pkl=None,\n        outfile=None,\n        target_template_frac=0.1,\n        max_target_frac_obs=0.25,\n        min_template_number=10,\n        max_template_number=1000,\n        max_rms=0.15,\n        max_mult_above_magmad=1.5,\n        max_mult_above_mageta=1.5,\n        xieta_bins=20,\n        mag_bandpass='sdssr',\n        custom_bandpasses=None,\n        mag_bright_limit=10.0,\n        mag_faint_limit=12.0,\n        process_template_lcs=True,\n        template_sigclip=5.0,\n        template_interpolate='nearest',\n        lcformat='hat-sql',\n        lcformatdir=None,\n        timecols=None,\n        magcols=None,\n        errcols=None,\n        nworkers=NCPUS,\n        maxworkertasks=1000,\n):\n    '''This selects template objects for TFA.\n\n    Selection criteria for TFA template ensemble objects:\n\n    - not variable: use a poly fit to the mag-MAD relation and eta-normal\n      variability index to get nonvar objects\n\n    - not more than 10% of the total number of objects in the field or\n      `max_tfa_templates` at most\n\n    - allow shuffling of the templates if the target ends up in them\n\n    - nothing with less than the median number of observations in the field\n\n    - sigma-clip the input time series observations\n\n    - TODO: uniform sampling in tangent plane coordinates (we'll need ra and\n      decl)\n\n    This also determines the effective cadence that all TFA LCs will be binned\n    to as the template LC with the largest number of non-nan observations will\n    be used. All template LCs will be renormed to zero.\n\n    Parameters\n    ----------\n\n    lclist : list of str\n        This is a list of light curves to use as input to generate the template\n        set.\n\n    lcinfo_pkl : str or None\n        If provided, is a file path to a pickle file created by this function on\n        a previous run containing the LC information. This will be loaded\n        directly instead of having to re-run LC info collection.\n\n    outfile : str or None\n        This is the pickle filename to which the TFA template list will be\n        written to. If None, a default file name will be used for this.\n\n    target_template_frac : float\n        This is the fraction of total objects in lclist to use for the number of\n        templates.\n\n    max_target_frac_obs : float\n        This sets the number of templates to generate if the number of\n        observations for the light curves is smaller than the number of objects\n        in the collection. The number of templates will be set to this fraction\n        of the number of observations if this is the case.\n\n    min_template_number : int\n        This is the minimum number of templates to generate.\n\n    max_template_number : int\n        This is the maximum number of templates to generate. If\n        `target_template_frac` times the number of objects is greater than\n        `max_template_number`, only `max_template_number` templates will be\n        used.\n\n    max_rms : float\n        This is the maximum light curve RMS for an object to consider it as a\n        possible template ensemble member.\n\n    max_mult_above_magmad : float\n        This is the maximum multiplier above the mag-RMS fit to consider an\n        object as variable and thus not part of the template ensemble.\n\n    max_mult_above_mageta : float\n        This is the maximum multiplier above the mag-eta (variable index) fit to\n        consider an object as variable and thus not part of the template\n        ensemble.\n\n    mag_bandpass : str\n        This sets the key in the light curve dict's objectinfo dict to use as\n        the canonical magnitude for the object and apply any magnitude limits\n        to.\n\n    custom_bandpasses : dict or None\n        This can be used to provide any custom band name keys to the star\n        feature collection function.\n\n    mag_bright_limit : float or list of floats\n        This sets the brightest mag (in the `mag_bandpass` filter) for a\n        potential member of the TFA template ensemble. If this is a single\n        float, the value will be used for all magcols. If this is a list of\n        floats with len = len(magcols), the specific bright limits will be used\n        for each magcol individually.\n\n    mag_faint_limit : float or list of floats\n        This sets the faintest mag (in the `mag_bandpass` filter) for a\n        potential member of the TFA template ensemble. If this is a single\n        float, the value will be used for all magcols. If this is a list of\n        floats with len = len(magcols), the specific faint limits will be used\n        for each magcol individually.\n\n    process_template_lcs : bool\n        If True, will reform the template light curves to the chosen\n        time-base. If False, will only select light curves for templates but not\n        process them. This is useful for initial exploration of how the template\n        LC are selected.\n\n    template_sigclip : float or sequence of floats or None\n        This sets the sigma-clip to be applied to the template light curves.\n\n    template_interpolate : str\n        This sets the kwarg to pass to `scipy.interpolate.interp1d` to set the\n        kind of interpolation to use when reforming light curves to the TFA\n        template timebase.\n\n    lcformat : str\n        This is the `formatkey` associated with your light curve format, which\n        you previously passed in to the `lcproc.register_lcformat`\n        function. This will be used to look up how to find and read the light\n        curves specified in `basedir` or `use_list_of_filenames`.\n\n    lcformatdir : str or None\n        If this is provided, gives the path to a directory when you've stored\n        your lcformat description JSONs, other than the usual directories lcproc\n        knows to search for them in. Use this along with `lcformat` to specify\n        an LC format JSON file that's not currently registered with lcproc.\n\n    timecols : list of str or None\n        The timecol keys to use from the lcdict in calculating the features.\n\n    magcols : list of str or None\n        The magcol keys to use from the lcdict in calculating the features.\n\n    errcols : list of str or None\n        The errcol keys to use from the lcdict in calculating the features.\n\n    nworkers : int\n        The number of parallel workers to launch.\n\n    maxworkertasks : int\n        The maximum number of tasks to run per worker before it is replaced by a\n        fresh one.\n\n    Returns\n    -------\n\n    dict\n        This function returns a dict that can be passed directly to\n        `apply_tfa_magseries` below. It can optionally produce a pickle with the\n        same dict, which can also be passed to that function.\n\n    '''\n\n    try:\n        formatinfo = get_lcformat(lcformat,\n                                  use_lcformat_dir=lcformatdir)\n        if formatinfo:\n            (dfileglob, readerfunc,\n             dtimecols, dmagcols, derrcols,\n             magsarefluxes, normfunc) = formatinfo\n        else:\n            LOGERROR(\"can't figure out the light curve format\")\n            return None\n    except Exception as e:\n        LOGEXCEPTION(\"can't figure out the light curve format\")\n        return None\n\n    # override the default timecols, magcols, and errcols\n    # using the ones provided to the function\n    if timecols is None:\n        timecols = dtimecols\n    if magcols is None:\n        magcols = dmagcols\n    if errcols is None:\n        errcols = derrcols\n\n    LOGINFO('collecting light curve information for %s objects in list...' %\n            len(lclist))\n\n    #\n    # check if we have cached results for this run\n    #\n\n    # case where we provide a cache info pkl directly\n    if lcinfo_pkl and os.path.exists(lcinfo_pkl):\n\n        with open(lcinfo_pkl,'rb') as infd:\n            results = pickle.load(infd)\n\n    # case where we don't have an info pickle or an outfile\n    elif ((not outfile) and\n          os.path.exists('tfa-collected-lcfinfo-%s.pkl' % lcformat)):\n\n        with open('tfa-collected-lcfinfo-%s.pkl' % lcformat, 'rb') as infd:\n            results = pickle.load(infd)\n\n    # case where we don't have an info pickle but do have an outfile\n    elif (outfile and os.path.exists('tfa-collected-lcfinfo-%s-%s' %\n                                     (lcformat, os.path.basename(outfile)))):\n\n        with open(\n                'tfa-collected-lcinfo-%s-%s' %\n                (lcformat, os.path.basename(outfile)),\n                'rb'\n        ) as infd:\n            results = pickle.load(infd)\n\n    # case where we have to redo the LC info collection\n    else:\n\n        # first, we'll collect the light curve info\n        tasks = [(x, lcformat, lcformat,\n                  timecols, magcols, errcols,\n                  custom_bandpasses) for x in lclist]\n\n        pool = mp.Pool(nworkers, maxtasksperchild=maxworkertasks)\n        results = pool.map(_collect_tfa_stats, tasks)\n        pool.close()\n        pool.join()\n\n        # save these results so we don't have to redo if something breaks here\n        if not outfile:\n            with open('tfa-collected-lcinfo-%s.pkl' % lcformat,'wb') as outfd:\n                pickle.dump(results, outfd, pickle.HIGHEST_PROTOCOL)\n        else:\n            with open(\n                    'tfa-collected-lcinfo-%s-%s' %\n                    (lcformat, os.path.basename(outfile)),\n                    'wb'\n            ) as outfd:\n                pickle.dump(results, outfd, pickle.HIGHEST_PROTOCOL)\n\n    #\n    # now, go through the light curve information\n    #\n\n    # find the center RA and center DEC -> median of all LC RAs and DECs\n    all_ras = np.array([res['ra'] for res in results])\n    all_decls = np.array([res['decl'] for res in results])\n    center_ra = np.nanmedian(all_ras)\n    center_decl = np.nanmedian(all_decls)\n\n    outdict = {\n        'timecols':[],\n        'magcols':[],\n        'errcols':[],\n        'center_ra':center_ra,\n        'center_decl':center_decl,\n    }\n\n    # for each magcol, we'll generate a separate template list\n    for tcol, mcol, ecol in zip(timecols, magcols, errcols):\n\n        if '.' in tcol:\n            tcolget = tcol.split('.')\n        else:\n            tcolget = [tcol]\n\n\n        # these are the containers for possible template collection LC info\n        (lcmag, lcmad, lceta,\n         lcndet, lcobj, lcfpaths,\n         lcra, lcdecl) = [], [], [], [], [], [], [], []\n\n        outdict['timecols'].append(tcol)\n        outdict['magcols'].append(mcol)\n        outdict['errcols'].append(ecol)\n\n        # add to the collection of all light curves\n        outdict[mcol] = {'collection':{'mag':[],\n                                       'mad':[],\n                                       'eta':[],\n                                       'ndet':[],\n                                       'obj':[],\n                                       'lcf':[],\n                                       'ra':[],\n                                       'decl':[]}}\n\n        LOGINFO('magcol: %s, collecting prospective template LC info...' %\n                mcol)\n\n\n        # collect the template LCs for this magcol\n        for result in results:\n\n            # we'll only append objects that have all of these elements\n            try:\n\n                thismag = result['colorfeat'][mag_bandpass]\n                thismad = result[mcol]['mad']\n                thiseta = result[mcol]['eta_normal']\n                thisndet = result[mcol]['ndet']\n                thisobj = result['objectid']\n                thislcf = result['lcfpath']\n                thisra = result['ra']\n                thisdecl = result['decl']\n\n                outdict[mcol]['collection']['mag'].append(thismag)\n                outdict[mcol]['collection']['mad'].append(thismad)\n                outdict[mcol]['collection']['eta'].append(thiseta)\n                outdict[mcol]['collection']['ndet'].append(thisndet)\n                outdict[mcol]['collection']['obj'].append(thisobj)\n                outdict[mcol]['collection']['lcf'].append(thislcf)\n                outdict[mcol]['collection']['ra'].append(thisra)\n                outdict[mcol]['collection']['decl'].append(thisdecl)\n\n\n                # check if we have more than one bright or faint limit elem\n                if isinstance(mag_bright_limit, (list, tuple)):\n                    use_bright_maglim = mag_bright_limit[\n                        magcols.index(mcol)\n                    ]\n                else:\n                    use_bright_maglim = mag_bright_limit\n                if isinstance(mag_faint_limit, (list, tuple)):\n                    use_faint_maglim = mag_faint_limit[\n                        magcols.index(mcol)\n                    ]\n                else:\n                    use_faint_maglim = mag_faint_limit\n\n                # make sure the object lies in the mag limits and RMS limits we\n                # set before to try to accept it into the TFA ensemble\n                if ((use_bright_maglim < thismag < use_faint_maglim) and\n                    (1.4826*thismad < max_rms)):\n\n                    lcmag.append(thismag)\n                    lcmad.append(thismad)\n                    lceta.append(thiseta)\n                    lcndet.append(thisndet)\n                    lcobj.append(thisobj)\n                    lcfpaths.append(thislcf)\n                    lcra.append(thisra)\n                    lcdecl.append(thisdecl)\n\n            except Exception as e:\n                pass\n\n        # make sure we have enough LCs to work on\n        if len(lcobj) >= min_template_number:\n\n            LOGINFO('magcol: %s, %s objects eligible for '\n                    'template selection after filtering on mag '\n                    'limits (%s, %s) and max RMS (%s)' %\n                    (mcol, len(lcobj),\n                     mag_bright_limit, mag_faint_limit, max_rms))\n\n            lcmag = np.array(lcmag)\n            lcmad = np.array(lcmad)\n            lceta = np.array(lceta)\n            lcndet = np.array(lcndet)\n            lcobj = np.array(lcobj)\n            lcfpaths = np.array(lcfpaths)\n            lcra = np.array(lcra)\n            lcdecl = np.array(lcdecl)\n\n            sortind = np.argsort(lcmag)\n            lcmag = lcmag[sortind]\n            lcmad = lcmad[sortind]\n            lceta = lceta[sortind]\n            lcndet = lcndet[sortind]\n            lcobj = lcobj[sortind]\n            lcfpaths = lcfpaths[sortind]\n            lcra = lcra[sortind]\n            lcdecl = lcdecl[sortind]\n\n            # 1. get the mag-MAD relation\n\n            # this is needed for spline fitting\n            # should take care of the pesky 'x must be strictly increasing' bit\n            splfit_ind = np.diff(lcmag) > 0.0\n            splfit_ind = np.concatenate((np.array([True]), splfit_ind))\n\n            fit_lcmag = lcmag[splfit_ind]\n            fit_lcmad = lcmad[splfit_ind]\n            fit_lceta = lceta[splfit_ind]\n\n            magmadfit = np.poly1d(np.polyfit(\n                fit_lcmag,\n                fit_lcmad,\n                2\n            ))\n            magmadind = lcmad/magmadfit(lcmag) < max_mult_above_magmad\n\n            # 2. get the mag-eta relation\n            magetafit = np.poly1d(np.polyfit(\n                fit_lcmag,\n                fit_lceta,\n                2\n            ))\n            magetaind = magetafit(lcmag)/lceta < max_mult_above_mageta\n\n            # 3. get the median ndet\n            median_ndet = np.median(lcndet)\n            ndetind = lcndet >= median_ndet\n\n            # form the final template ensemble\n            templateind = magmadind & magetaind & ndetind\n\n            # check again if we have enough LCs in the template\n            if templateind.sum() >= min_template_number:\n\n                LOGINFO('magcol: %s, %s objects selectable for TFA templates' %\n                        (mcol, templateind.sum()))\n\n                templatemag = lcmag[templateind]\n                templatemad = lcmad[templateind]\n                templateeta = lceta[templateind]\n                templatendet = lcndet[templateind]\n                templateobj = lcobj[templateind]\n                templatelcf = lcfpaths[templateind]\n                templatera = lcra[templateind]\n                templatedecl = lcdecl[templateind]\n\n                # now, check if we have no more than the required fraction of\n                # TFA templates\n                target_number_templates = int(target_template_frac*len(results))\n\n                if target_number_templates > max_template_number:\n                    target_number_templates = max_template_number\n\n                LOGINFO('magcol: %s, selecting %s TFA templates randomly' %\n                        (mcol, target_number_templates))\n\n                # FIXME: how do we select uniformly in xi-eta?\n                # 1. 2D histogram the data into binsize (nx, ny)\n                # 2. random uniform select from 0 to nx-1, 0 to ny-1\n                # 3. pick object from selected bin\n                # 4. continue until we have target_number_templates\n                # 5. make sure the same object isn't picked twice\n\n                # get the xi-eta\n                template_cxi, template_ceta = coordutils.xieta_from_radecl(\n                    templatera,\n                    templatedecl,\n                    center_ra,\n                    center_decl\n                )\n\n                cxi_bins = np.linspace(template_cxi.min(),\n                                       template_cxi.max(),\n                                       num=xieta_bins)\n                ceta_bins = np.linspace(template_ceta.min(),\n                                        template_ceta.max(),\n                                        num=xieta_bins)\n\n                digitized_cxi_inds = np.digitize(template_cxi, cxi_bins)\n                digitized_ceta_inds = np.digitize(template_ceta, ceta_bins)\n\n                # pick target_number_templates indexes out of the bins\n                targetind = npr.choice(xieta_bins,\n                                       target_number_templates,\n                                       replace=True)\n\n                # put together the template lists\n                selected_template_obj = []\n                selected_template_lcf = []\n                selected_template_ndet = []\n                selected_template_ra = []\n                selected_template_decl = []\n                selected_template_mag = []\n                selected_template_mad = []\n                selected_template_eta = []\n\n                for ind in targetind:\n\n                    pass\n\n\n                # select random uniform objects from the template candidates\n                targetind = npr.choice(templateobj.size,\n                                       target_number_templates,\n                                       replace=False)\n\n                templatemag = templatemag[targetind]\n                templatemad = templatemad[targetind]\n                templateeta = templateeta[targetind]\n                templatendet = templatendet[targetind]\n                templateobj = templateobj[targetind]\n                templatelcf = templatelcf[targetind]\n                templatera = templatera[targetind]\n                templatedecl = templatedecl[targetind]\n\n                # get the max ndet so far to use that LC as the timebase\n                maxndetind = templatendet == templatendet.max()\n                timebaselcf = templatelcf[maxndetind][0]\n                timebasendet = templatendet[maxndetind][0]\n\n                LOGINFO('magcol: %s, selected %s as template time '\n                        'base LC with %s observations' %\n                        (mcol, timebaselcf, timebasendet))\n\n                if process_template_lcs:\n\n                    timebaselcdict = readerfunc(timebaselcf)\n\n                    if ( (isinstance(timebaselcdict, (list, tuple))) and\n                         (isinstance(timebaselcdict[0], dict)) ):\n                        timebaselcdict = timebaselcdict[0]\n\n                    # this is the timebase to use for all of the templates\n                    timebase = _dict_get(timebaselcdict, tcolget)\n\n                else:\n                    timebase = None\n\n                # also check if the number of templates is longer than the\n                # actual timebase of the observations. this will cause issues\n                # with overcorrections and will probably break TFA\n                if target_number_templates > timebasendet:\n\n                    LOGWARNING('The number of TFA templates (%s) is '\n                               'larger than the number of observations '\n                               'of the time base (%s). This will likely '\n                               'overcorrect all light curves to a '\n                               'constant level. '\n                               'Will use up to %s x timebase ndet '\n                               'templates instead' %\n                               (target_number_templates,\n                                timebasendet,\n                                max_target_frac_obs))\n\n                    # regen the templates based on the new number\n                    newmaxtemplates = int(max_target_frac_obs*timebasendet)\n\n                    # choose this number out of the already chosen templates\n                    # randomly\n\n                    LOGWARNING('magcol: %s, re-selecting %s TFA '\n                               'templates randomly' %\n                               (mcol, newmaxtemplates))\n\n                    # FIXME: how do we select uniformly in ra-decl?\n                    # 1. 2D histogram the data into binsize (nx, ny)\n                    # 2. random uniform select from 0 to nx-1, 0 to ny-1\n                    # 3. pick object from selected bin\n                    # 4. continue until we have target_number_templates\n                    # 5. make sure the same object isn't picked twice\n\n                    # select random uniform objects from the template candidates\n                    targetind = npr.choice(templateobj.size,\n                                           newmaxtemplates,\n                                           replace=False)\n\n                    templatemag = templatemag[targetind]\n                    templatemad = templatemad[targetind]\n                    templateeta = templateeta[targetind]\n                    templatendet = templatendet[targetind]\n                    templateobj = templateobj[targetind]\n                    templatelcf = templatelcf[targetind]\n                    templatera = templatera[targetind]\n                    templatedecl = templatedecl[targetind]\n\n                    # get the max ndet so far to use that LC as the timebase\n                    maxndetind = templatendet == templatendet.max()\n                    timebaselcf = templatelcf[maxndetind][0]\n                    timebasendet = templatendet[maxndetind][0]\n                    LOGWARNING('magcol: %s, re-selected %s as template time '\n                               'base LC with %s observations' %\n                               (mcol, timebaselcf, timebasendet))\n\n                    if process_template_lcs:\n\n                        timebaselcdict = readerfunc(timebaselcf)\n\n                        if ( (isinstance(timebaselcdict, (list, tuple))) and\n                             (isinstance(timebaselcdict[0], dict)) ):\n                            timebaselcdict = timebaselcdict[0]\n\n                        # this is the timebase to use for all of the templates\n                        timebase = _dict_get(timebaselcdict, tcolget)\n\n                    else:\n\n                        timebase = None\n\n                #\n                # end of check for ntemplates > timebase ndet\n                #\n\n                if process_template_lcs:\n\n                    LOGINFO('magcol: %s, reforming TFA template LCs to '\n                            ' chosen timebase...' % mcol)\n\n                    # reform all template LCs to this time base, normalize to\n                    # zero, and sigclip as requested. this is a parallel op\n                    # first, we'll collect the light curve info\n                    tasks = [(x, lcformat, lcformatdir,\n                              tcol, mcol, ecol,\n                              timebase, template_interpolate,\n                              template_sigclip) for x\n                             in templatelcf]\n\n                    pool = mp.Pool(nworkers, maxtasksperchild=maxworkertasks)\n                    reform_results = pool.map(_reform_templatelc_for_tfa, tasks)\n                    pool.close()\n                    pool.join()\n\n                    # generate a 2D array for the template magseries with\n                    # dimensions = (n_objects, n_lcpoints)\n                    template_magseries = np.array([x['mags']\n                                                   for x in reform_results])\n                    template_errseries = np.array([x['errs']\n                                                   for x in reform_results])\n\n                else:\n                    template_magseries = None\n                    template_errseries = None\n\n                # put everything into a templateinfo dict for this magcol\n                outdict[mcol].update({\n                    'timebaselcf':timebaselcf,\n                    'timebase':timebase,\n                    'trendfits':{'mag-mad':magmadfit,\n                                 'mag-eta':magetafit},\n                    'template_objects':templateobj,\n                    'template_ra':templatera,\n                    'template_decl':templatedecl,\n                    'template_mag':templatemag,\n                    'template_mad':templatemad,\n                    'template_eta':templateeta,\n                    'template_ndet':templatendet,\n                    'template_magseries':template_magseries,\n                    'template_errseries':template_errseries\n                })\n\n                # make a KDTree on the template coordinates\n                outdict[mcol]['template_radecl_kdtree'] = (\n                    coordutils.make_kdtree(\n                        templatera, templatedecl\n                    )\n                )\n\n            # if we don't have enough, return nothing for this magcol\n            else:\n                LOGERROR('not enough objects meeting requested '\n                         'MAD, eta, ndet conditions to '\n                         'select templates for magcol: %s' % mcol)\n                continue\n\n        else:\n\n            LOGERROR('nobjects: %s, not enough in requested mag range to '\n                     'select templates for magcol: %s' % (len(lcobj),mcol))\n\n            continue\n\n        # make the plots for mag-MAD/mag-eta relation and fits used\n        plt.plot(lcmag, lcmad, marker='o', linestyle='none', ms=1.0)\n        modelmags = np.linspace(lcmag.min(), lcmag.max(), num=1000)\n        plt.plot(modelmags, outdict[mcol]['trendfits']['mag-mad'](modelmags))\n        plt.yscale('log')\n        plt.xlabel('catalog magnitude')\n        plt.ylabel('light curve MAD')\n        plt.title('catalog mag vs. light curve MAD and fit')\n        plt.savefig('catmag-%s-lcmad-fit.png' % mcol,\n                    bbox_inches='tight')\n        plt.close('all')\n\n        plt.plot(lcmag, lceta, marker='o', linestyle='none', ms=1.0)\n        modelmags = np.linspace(lcmag.min(), lcmag.max(), num=1000)\n        plt.plot(modelmags, outdict[mcol]['trendfits']['mag-eta'](modelmags))\n        plt.yscale('log')\n        plt.xlabel('catalog magnitude')\n        plt.ylabel('light curve eta variable index')\n        plt.title('catalog mag vs. light curve eta and fit')\n        plt.savefig('catmag-%s-lceta-fit.png' % mcol,\n                    bbox_inches='tight')\n        plt.close('all')\n\n\n    #\n    # end of operating on each magcol\n    #\n\n    # save the templateinfo dict to a pickle if requested\n    if outfile:\n\n        if outfile.endswith('.gz'):\n            outfd = gzip.open(outfile,'wb')\n        else:\n            outfd = open(outfile,'wb')\n\n        with outfd:\n            pickle.dump(outdict, outfd, protocol=pickle.HIGHEST_PROTOCOL)\n\n    # return the templateinfo dict\n    return outdict", "12": "def get_recovered_variables_for_magbin(simbasedir,\n                                       magbinmedian,\n                                       stetson_stdev_min=2.0,\n                                       inveta_stdev_min=2.0,\n                                       iqr_stdev_min=2.0,\n                                       statsonly=True):\n    '''This runs variability selection for the given magbinmedian.\n\n    To generate a full recovery matrix over all magnitude bins, run this\n    function for each magbin over the specified stetson_stdev_min and\n    inveta_stdev_min grid.\n\n    Parameters\n    ----------\n\n    simbasedir : str\n        The input directory of fake LCs.\n\n    magbinmedian : float\n        The magbin to run the variable recovery for. This is an item from the\n        dict from `simbasedir/fakelcs-info.pkl: `fakelcinfo['magrms'][magcol]`\n        list for each magcol and designates which magbin to get the recovery\n        stats for.\n\n    stetson_stdev_min : float\n        The minimum sigma above the trend in the Stetson J variability index\n        distribution for this magbin to use to consider objects as variable.\n\n    inveta_stdev_min : float\n        The minimum sigma above the trend in the 1/eta variability index\n        distribution for this magbin to use to consider objects as variable.\n\n    iqr_stdev_min : float\n        The minimum sigma above the trend in the IQR variability index\n        distribution for this magbin to use to consider objects as variable.\n\n    statsonly : bool\n        If this is True, only the final stats will be returned. If False, the\n        full arrays used to generate the stats will also be returned.\n\n    Returns\n    -------\n\n    dict\n        The returned dict contains statistics for this magbin and if requested,\n        the full arrays used to calculate the statistics.\n\n    '''\n\n\n    # get the info from the simbasedir\n    with open(os.path.join(simbasedir, 'fakelcs-info.pkl'),'rb') as infd:\n        siminfo = pickle.load(infd)\n\n    objectids = siminfo['objectid']\n    varflags = siminfo['isvariable']\n    sdssr = siminfo['sdssr']\n\n    # get the column defs for the fakelcs\n    timecols = siminfo['timecols']\n    magcols = siminfo['magcols']\n    errcols = siminfo['errcols']\n\n    # register the fakelc pklc as a custom lcproc format\n    # now we should be able to use all lcproc functions correctly\n    fakelc_formatkey = 'fake-%s' % siminfo['lcformat']\n    lcproc.register_lcformat(\n        fakelc_formatkey,\n        '*-fakelc.pkl',\n        timecols,\n        magcols,\n        errcols,\n        'astrobase.lcproc',\n        '_read_pklc',\n        magsarefluxes=siminfo['magsarefluxes']\n    )\n\n    # make the output directory if it doesn't exit\n    outdir = os.path.join(simbasedir, 'recvar-threshold-pkls')\n    if not os.path.exists(outdir):\n        os.mkdir(outdir)\n\n    # run the variability search\n    varfeaturedir = os.path.join(simbasedir, 'varfeatures')\n    varthreshinfof = os.path.join(\n        outdir,\n        'varthresh-magbinmed%.2f-stet%.2f-inveta%.2f.pkl' % (magbinmedian,\n                                                             stetson_stdev_min,\n                                                             inveta_stdev_min)\n    )\n    varthresh = varthreshold.variability_threshold(\n        varfeaturedir,\n        varthreshinfof,\n        lcformat=fakelc_formatkey,\n        min_stetj_stdev=stetson_stdev_min,\n        min_inveta_stdev=inveta_stdev_min,\n        min_iqr_stdev=iqr_stdev_min,\n        verbose=False\n    )\n\n    # get the magbins from the varthresh info\n    magbins = varthresh['magbins']\n\n    # get the magbininds\n    magbininds = np.digitize(sdssr, magbins)\n\n    # bin the objects according to these magbins\n    binned_objectids = []\n    binned_actualvars = []\n    binned_actualnotvars = []\n\n    # go through all the mag bins and bin up the objectids, actual variables,\n    # and actual not-variables\n    for mbinind, _magi in zip(np.unique(magbininds),\n                              range(len(magbins)-1)):\n\n        thisbinind = np.where(magbininds == mbinind)\n\n        thisbin_objectids = objectids[thisbinind]\n        thisbin_varflags = varflags[thisbinind]\n\n        thisbin_actualvars = thisbin_objectids[thisbin_varflags]\n        thisbin_actualnotvars = thisbin_objectids[~thisbin_varflags]\n\n        binned_objectids.append(thisbin_objectids)\n        binned_actualvars.append(thisbin_actualvars)\n        binned_actualnotvars.append(thisbin_actualnotvars)\n\n\n    # this is the output dict\n    recdict = {\n        'simbasedir':simbasedir,\n        'timecols':timecols,\n        'magcols':magcols,\n        'errcols':errcols,\n        'magsarefluxes':siminfo['magsarefluxes'],\n        'stetj_min_stdev':stetson_stdev_min,\n        'inveta_min_stdev':inveta_stdev_min,\n        'iqr_min_stdev':iqr_stdev_min,\n        'magbinmedian':magbinmedian,\n    }\n\n\n    # now, for each magcol, find the magbin corresponding to magbinmedian, and\n    # get its stats\n    for magcol in magcols:\n\n        # this is the index of the matching magnitude bin for the magbinmedian\n        # provided\n        magbinind = np.where(\n            np.array(varthresh[magcol]['binned_sdssr_median']) == magbinmedian\n        )\n\n        magbinind = np.asscalar(magbinind[0])\n\n        # get the objectids, actual vars and actual notvars in this magbin\n        thisbin_objectids = binned_objectids[magbinind]\n        thisbin_actualvars = binned_actualvars[magbinind]\n        thisbin_actualnotvars = binned_actualnotvars[magbinind]\n\n        # stetson recovered variables in this magbin\n        stet_recoveredvars = varthresh[magcol][\n            'binned_objectids_thresh_stetsonj'\n        ][magbinind]\n\n        # calculate TP, FP, TN, FN\n        stet_recoverednotvars = np.setdiff1d(thisbin_objectids,\n                                             stet_recoveredvars)\n\n        stet_truepositives = np.intersect1d(stet_recoveredvars,\n                                            thisbin_actualvars)\n        stet_falsepositives = np.intersect1d(stet_recoveredvars,\n                                             thisbin_actualnotvars)\n        stet_truenegatives = np.intersect1d(stet_recoverednotvars,\n                                            thisbin_actualnotvars)\n        stet_falsenegatives = np.intersect1d(stet_recoverednotvars,\n                                             thisbin_actualvars)\n\n        # calculate stetson recall, precision, Matthews correl coeff\n        stet_recall = recall(stet_truepositives.size,\n                             stet_falsenegatives.size)\n\n        stet_precision = precision(stet_truepositives.size,\n                                   stet_falsepositives.size)\n\n        stet_mcc = matthews_correl_coeff(stet_truepositives.size,\n                                         stet_truenegatives.size,\n                                         stet_falsepositives.size,\n                                         stet_falsenegatives.size)\n\n\n        # inveta recovered variables in this magbin\n        inveta_recoveredvars = varthresh[magcol][\n            'binned_objectids_thresh_inveta'\n        ][magbinind]\n        inveta_recoverednotvars = np.setdiff1d(thisbin_objectids,\n                                               inveta_recoveredvars)\n\n        inveta_truepositives = np.intersect1d(inveta_recoveredvars,\n                                              thisbin_actualvars)\n        inveta_falsepositives = np.intersect1d(inveta_recoveredvars,\n                                               thisbin_actualnotvars)\n        inveta_truenegatives = np.intersect1d(inveta_recoverednotvars,\n                                              thisbin_actualnotvars)\n        inveta_falsenegatives = np.intersect1d(inveta_recoverednotvars,\n                                               thisbin_actualvars)\n\n        # calculate inveta recall, precision, Matthews correl coeff\n        inveta_recall = recall(inveta_truepositives.size,\n                               inveta_falsenegatives.size)\n\n        inveta_precision = precision(inveta_truepositives.size,\n                                     inveta_falsepositives.size)\n\n        inveta_mcc = matthews_correl_coeff(inveta_truepositives.size,\n                                           inveta_truenegatives.size,\n                                           inveta_falsepositives.size,\n                                           inveta_falsenegatives.size)\n\n\n        # iqr recovered variables in this magbin\n        iqr_recoveredvars = varthresh[magcol][\n            'binned_objectids_thresh_iqr'\n        ][magbinind]\n        iqr_recoverednotvars = np.setdiff1d(thisbin_objectids,\n                                            iqr_recoveredvars)\n\n        iqr_truepositives = np.intersect1d(iqr_recoveredvars,\n                                           thisbin_actualvars)\n        iqr_falsepositives = np.intersect1d(iqr_recoveredvars,\n                                            thisbin_actualnotvars)\n        iqr_truenegatives = np.intersect1d(iqr_recoverednotvars,\n                                           thisbin_actualnotvars)\n        iqr_falsenegatives = np.intersect1d(iqr_recoverednotvars,\n                                            thisbin_actualvars)\n\n        # calculate iqr recall, precision, Matthews correl coeff\n        iqr_recall = recall(iqr_truepositives.size,\n                            iqr_falsenegatives.size)\n\n        iqr_precision = precision(iqr_truepositives.size,\n                                  iqr_falsepositives.size)\n\n        iqr_mcc = matthews_correl_coeff(iqr_truepositives.size,\n                                        iqr_truenegatives.size,\n                                        iqr_falsepositives.size,\n                                        iqr_falsenegatives.size)\n\n\n        # calculate the items missed by one method but found by the other\n        # methods\n        stet_missed_inveta_found = np.setdiff1d(inveta_truepositives,\n                                                stet_truepositives)\n        stet_missed_iqr_found = np.setdiff1d(iqr_truepositives,\n                                             stet_truepositives)\n\n        inveta_missed_stet_found = np.setdiff1d(stet_truepositives,\n                                                inveta_truepositives)\n        inveta_missed_iqr_found = np.setdiff1d(iqr_truepositives,\n                                               inveta_truepositives)\n\n        iqr_missed_stet_found = np.setdiff1d(stet_truepositives,\n                                             iqr_truepositives)\n        iqr_missed_inveta_found = np.setdiff1d(inveta_truepositives,\n                                               iqr_truepositives)\n\n        if not statsonly:\n\n            recdict[magcol] = {\n                # stetson J alone\n                'stet_recoveredvars':stet_recoveredvars,\n                'stet_truepositives':stet_truepositives,\n                'stet_falsepositives':stet_falsepositives,\n                'stet_truenegatives':stet_truenegatives,\n                'stet_falsenegatives':stet_falsenegatives,\n                'stet_precision':stet_precision,\n                'stet_recall':stet_recall,\n                'stet_mcc':stet_mcc,\n                # inveta alone\n                'inveta_recoveredvars':inveta_recoveredvars,\n                'inveta_truepositives':inveta_truepositives,\n                'inveta_falsepositives':inveta_falsepositives,\n                'inveta_truenegatives':inveta_truenegatives,\n                'inveta_falsenegatives':inveta_falsenegatives,\n                'inveta_precision':inveta_precision,\n                'inveta_recall':inveta_recall,\n                'inveta_mcc':inveta_mcc,\n                # iqr alone\n                'iqr_recoveredvars':iqr_recoveredvars,\n                'iqr_truepositives':iqr_truepositives,\n                'iqr_falsepositives':iqr_falsepositives,\n                'iqr_truenegatives':iqr_truenegatives,\n                'iqr_falsenegatives':iqr_falsenegatives,\n                'iqr_precision':iqr_precision,\n                'iqr_recall':iqr_recall,\n                'iqr_mcc':iqr_mcc,\n                # true positive variables missed by one method but picked up by\n                # the others\n                'stet_missed_inveta_found':stet_missed_inveta_found,\n                'stet_missed_iqr_found':stet_missed_iqr_found,\n                'inveta_missed_stet_found':inveta_missed_stet_found,\n                'inveta_missed_iqr_found':inveta_missed_iqr_found,\n                'iqr_missed_stet_found':iqr_missed_stet_found,\n                'iqr_missed_inveta_found':iqr_missed_inveta_found,\n                # bin info\n                'actual_variables':thisbin_actualvars,\n                'actual_nonvariables':thisbin_actualnotvars,\n                'all_objectids':thisbin_objectids,\n                'magbinind':magbinind,\n\n            }\n\n        # if statsonly is set, then we only return the numbers but not the\n        # arrays themselves\n        else:\n\n            recdict[magcol] = {\n                # stetson J alone\n                'stet_recoveredvars':stet_recoveredvars.size,\n                'stet_truepositives':stet_truepositives.size,\n                'stet_falsepositives':stet_falsepositives.size,\n                'stet_truenegatives':stet_truenegatives.size,\n                'stet_falsenegatives':stet_falsenegatives.size,\n                'stet_precision':stet_precision,\n                'stet_recall':stet_recall,\n                'stet_mcc':stet_mcc,\n                # inveta alone\n                'inveta_recoveredvars':inveta_recoveredvars.size,\n                'inveta_truepositives':inveta_truepositives.size,\n                'inveta_falsepositives':inveta_falsepositives.size,\n                'inveta_truenegatives':inveta_truenegatives.size,\n                'inveta_falsenegatives':inveta_falsenegatives.size,\n                'inveta_precision':inveta_precision,\n                'inveta_recall':inveta_recall,\n                'inveta_mcc':inveta_mcc,\n                # iqr alone\n                'iqr_recoveredvars':iqr_recoveredvars.size,\n                'iqr_truepositives':iqr_truepositives.size,\n                'iqr_falsepositives':iqr_falsepositives.size,\n                'iqr_truenegatives':iqr_truenegatives.size,\n                'iqr_falsenegatives':iqr_falsenegatives.size,\n                'iqr_precision':iqr_precision,\n                'iqr_recall':iqr_recall,\n                'iqr_mcc':iqr_mcc,\n                # true positive variables missed by one method but picked up by\n                # the others\n                'stet_missed_inveta_found':stet_missed_inveta_found.size,\n                'stet_missed_iqr_found':stet_missed_iqr_found.size,\n                'inveta_missed_stet_found':inveta_missed_stet_found.size,\n                'inveta_missed_iqr_found':inveta_missed_iqr_found.size,\n                'iqr_missed_stet_found':iqr_missed_stet_found.size,\n                'iqr_missed_inveta_found':iqr_missed_inveta_found.size,\n                # bin info\n                'actual_variables':thisbin_actualvars.size,\n                'actual_nonvariables':thisbin_actualnotvars.size,\n                'all_objectids':thisbin_objectids.size,\n                'magbinind':magbinind,\n            }\n\n\n    #\n    # done with per magcol\n    #\n\n    return recdict", "13": "def update_assembly(data):\n    \"\"\" \n    Create a new Assembly() and convert as many of our old params to the new\n    version as we can. Also report out any parameters that are removed\n    and what their values are. \n    \"\"\"\n\n    print(\"##############################################################\")\n    print(\"Updating assembly to current version\")\n    ## New assembly object to update pdate from.\n    new_assembly = ip.Assembly(\"update\", quiet=True)\n\n    ## Hackersonly dict gets automatically overwritten\n    ## Always use the current version for params in this dict.\n    data._hackersonly = deepcopy(new_assembly._hackersonly)\n\n    new_params = set(new_assembly.paramsdict.keys())\n\n    my_params = set(data.paramsdict.keys())\n\n    ## Find all params in loaded assembly that aren't in the new assembly.\n    ## Make a new dict that doesn't include anything in removed_params\n    removed_params = my_params.difference(new_params)\n    for i in removed_params:\n        print(\"Removing parameter: {} = {}\".format(i, data.paramsdict[i]))\n        \n    ## Find all params that are in the new paramsdict and not in the old one.\n    ## If the set isn't emtpy then we create a new dictionary based on the new\n    ## assembly parameters and populated with currently loaded assembly values.\n    ## Conditioning on not including any removed params. Magic.\n    added_params = new_params.difference(my_params)\n    for i in added_params:\n        print(\"Adding parameter: {} = {}\".format(i, new_assembly.paramsdict[i]))\n\n    print(\"\\nPlease take note of these changes. Every effort is made to\\n\"\\\n            +\"ensure compatibility across versions of ipyrad. See online\\n\"\\\n            +\"documentation for further details about new parameters.\")\n    time.sleep(5)\n    print(\"##############################################################\")\n    \n    if added_params:\n        for i in data.paramsdict:\n            if i not in removed_params:\n                new_assembly.paramsdict[i] = data.paramsdict[i]\n        data.paramsdict = deepcopy(new_assembly.paramsdict)\n\n    data.save()\n    return data", "14": "def join(self, password=None, history_maxchars = None,\n            history_maxstanzas = None, history_seconds = None, history_since = None):\n        \"\"\"\n        Send a join request for the room.\n\n        :Parameters:\n            - `password`: password to the room.\n            - `history_maxchars`: limit of the total number of characters in\n              history.\n            - `history_maxstanzas`: limit of the total number of messages in\n              history.\n            - `history_seconds`: send only messages received in the last\n              `history_seconds` seconds.\n            - `history_since`: Send only the messages received since the\n              dateTime specified (UTC).\n        :Types:\n            - `password`: `unicode`\n            - `history_maxchars`: `int`\n            - `history_maxstanzas`: `int`\n            - `history_seconds`: `int`\n            - `history_since`: `datetime.datetime`\n        \"\"\"\n        if self.joined:\n            raise RuntimeError(\"Room is already joined\")\n        p=MucPresence(to_jid=self.room_jid)\n        p.make_join_request(password, history_maxchars, history_maxstanzas,\n                history_seconds, history_since)\n        self.manager.stream.send(p)", "15": "def _at_block_start(tc, line):\n        \"\"\"\n        Improve QTextCursor.atBlockStart to ignore spaces\n        \"\"\"\n        if tc.atBlockStart():\n            return True\n        column = tc.columnNumber()\n        indentation = len(line) - len(line.lstrip())\n        return column <= indentation", "16": "def _string(self):\n        \"\"\":return: the string from a :class:`io.StringIO`\"\"\"\n        file = StringIO()\n        self.__dump_to_file(file)\n        file.seek(0)\n        return file.read()", "17": "def plot_diff(self, graphing_library='matplotlib'):\n    \"\"\"\n    Generate CDF diff plots of the submetrics\n    \"\"\"\n    diff_datasource = sorted(set(self.reports[0].datasource) & set(self.reports[1].datasource))\n    graphed = False\n    for submetric in diff_datasource:\n      baseline_csv = naarad.utils.get_default_csv(self.reports[0].local_location, (submetric + '.percentiles'))\n      current_csv = naarad.utils.get_default_csv(self.reports[1].local_location, (submetric + '.percentiles'))\n      if (not (naarad.utils.is_valid_file(baseline_csv) & naarad.utils.is_valid_file(current_csv))):\n        continue\n      baseline_plot = PD(input_csv=baseline_csv, csv_column=1, series_name=submetric, y_label=submetric, precision=None, graph_height=600, graph_width=1200,\n                         graph_type='line', plot_label='baseline', x_label='Percentiles')\n      current_plot = PD(input_csv=current_csv, csv_column=1, series_name=submetric, y_label=submetric, precision=None, graph_height=600, graph_width=1200,\n                        graph_type='line', plot_label='current', x_label='Percentiles')\n      graphed, div_file = Diff.graphing_modules[graphing_library].graph_data_on_the_same_graph([baseline_plot, current_plot],\n                                                                                               os.path.join(self.output_directory, self.resource_path),\n                                                                                               self.resource_path, (submetric + '.diff'))\n      if graphed:\n        self.plot_files.append(div_file)\n    return True", "18": "def stop(ctx, yes):\n    \"\"\"Stop experiment.\n\n    Uses [Caching](/references/polyaxon-cli/#caching)\n\n    Examples:\n\n    \\b\n    ```bash\n    $ polyaxon experiment stop\n    ```\n\n    \\b\n    ```bash\n    $ polyaxon experiment -xp 2 stop\n    ```\n    \"\"\"\n    user, project_name, _experiment = get_project_experiment_or_local(ctx.obj.get('project'),\n                                                                      ctx.obj.get('experiment'))\n    if not yes and not click.confirm(\"Are sure you want to stop \"\n                                     \"experiment `{}`\".format(_experiment)):\n        click.echo('Existing without stopping experiment.')\n        sys.exit(0)\n\n    try:\n        PolyaxonClient().experiment.stop(user, project_name, _experiment)\n    except (PolyaxonHTTPError, PolyaxonShouldExitError, PolyaxonClientException) as e:\n        Printer.print_error('Could not stop experiment `{}`.'.format(_experiment))\n        Printer.print_error('Error message `{}`.'.format(e))\n        sys.exit(1)\n\n    Printer.print_success(\"Experiment is being stopped.\")", "19": "def create(cls, data, id_=None):\n        \"\"\"Create a deposit.\n\n        Initialize the follow information inside the deposit:\n\n        .. code-block:: python\n\n            deposit['_deposit'] = {\n                'id': pid_value,\n                'status': 'draft',\n                'owners': [user_id],\n                'created_by': user_id,\n            }\n\n        The deposit index is updated.\n\n        :param data: Input dictionary to fill the deposit.\n        :param id_: Default uuid for the deposit.\n        :returns: The new created deposit.\n        \"\"\"\n        data.setdefault('$schema', current_jsonschemas.path_to_url(\n            current_app.config['DEPOSIT_DEFAULT_JSONSCHEMA']\n        ))\n        if '_deposit' not in data:\n            id_ = id_ or uuid.uuid4()\n            cls.deposit_minter(id_, data)\n\n        data['_deposit'].setdefault('owners', list())\n        if current_user and current_user.is_authenticated:\n            creator_id = int(current_user.get_id())\n\n            if creator_id not in data['_deposit']['owners']:\n                data['_deposit']['owners'].append(creator_id)\n\n            data['_deposit']['created_by'] = creator_id\n\n        return super(Deposit, cls).create(data, id_=id_)", "20": "def getapplist(self):\n        \"\"\"\n        Get all accessibility application name that are currently running\n\n        @return: list of appliction name of string type on success.\n        @rtype: list\n        \"\"\"\n        app_list = []\n        # Update apps list, before parsing the list\n        self._update_apps()\n        for gui in self._running_apps:\n            name = gui.localizedName()\n            # default type was objc.pyobjc_unicode\n            # convert to Unicode, else exception is thrown\n            # TypeError: \"cannot marshal <type 'objc.pyobjc_unicode'> objects\"\n            try:\n                name = unicode(name)\n            except NameError:\n                name = str(name)\n            except UnicodeEncodeError:\n                pass\n            app_list.append(name)\n        # Return unique application list\n        return list(set(app_list))", "21": "def setcursorposition(self, window_name, object_name, cursor_position):\n        \"\"\"\n        Set cursor position\n        \n        @param window_name: Window name to type in, either full name,\n        LDTP's name convention, or a Unix glob.\n        @type window_name: string\n        @param object_name: Object name to type in, either full name,\n        LDTP's name convention, or a Unix glob. \n        @type object_name: string\n        @param cursor_position: Cursor position to be set\n        @type object_name: string\n\n        @return: 1 on success.\n        @rtype: integer\n        \"\"\"\n        object_handle = self._get_object_handle(window_name, object_name)\n        if not object_handle.AXEnabled:\n            raise LdtpServerException(u\"Object %s state disabled\" % object_name)\n        object_handle.AXSelectedTextRange.loc = cursor_position\n        return 1", "22": "def _is_compound_mfr_temperature_tuple(self, value):\n        \"\"\"Determines whether value is a tuple of the format\n        (compound(str), mfr(float), temperature(float)).\n\n        :param value: The value to be tested.\n\n        :returns: True or False\"\"\"\n\n        if not type(value) is tuple:\n            return False\n        elif not len(value) == 3:\n            return False\n        elif not type(value[0]) is str:\n            return False\n        elif not type(value[1]) is float and \\\n                not type(value[1]) is numpy.float64 and \\\n                not type(value[1]) is numpy.float32:\n            return False\n        elif not type(value[1]) is float and \\\n                not type(value[1]) is numpy.float64 and \\\n                not type(value[1]) is numpy.float32:\n            return False\n        else:\n            return True", "23": "def stop(self):\n        \"\"\"\n        Stops monitoring the predefined directory.\n        \"\"\"\n        with self._status_lock:\n            if self._running:\n                assert self._observer is not None\n                self._observer.stop()\n                self._running = False\n                self._origin_mapped_data = dict()", "24": "def read(self, request, pk=None):\n        \"\"\"\n        Mark the message as read (i.e. delete from inbox)\n        \"\"\"\n        from .settings import stored_messages_settings\n        backend = stored_messages_settings.STORAGE_BACKEND()\n\n        try:\n            backend.inbox_delete(request.user, pk)\n        except MessageDoesNotExist as e:\n            return Response(e.message, status='404')\n\n        return Response({'status': 'message marked as read'})", "25": "def create_empty_dataset(self, ds_name, dtype=np.float32):\n        \"\"\"\n        Creates a Dataset with unknown size.\n        Resize it before using.\n\n        :param ds_name: string\n\n        :param dtype: dtype\n        Datatype of the dataset\n\n        :return: h5py DataSet\n        \"\"\"\n        if ds_name in self._datasets:\n            return self._datasets[ds_name]\n\n        ds = self._group.create_dataset(ds_name, (1, 1), maxshape=None,\n                                        dtype=dtype)\n        self._datasets[ds_name] = ds\n\n        return ds", "26": "def update(course=False):\n    \"\"\"\n    Update the data of courses and or exercises from server.\n    \"\"\"\n    if course:\n        with Spinner.context(msg=\"Updated course metadata.\",\n                             waitmsg=\"Updating course metadata.\"):\n            for course in api.get_courses():\n                old = None\n                try:\n                    old = Course.get(Course.tid == course[\"id\"])\n                except peewee.DoesNotExist:\n                    old = None\n                if old:\n                    old.details_url = course[\"details_url\"]\n                    old.save()\n                    continue\n                Course.create(tid=course[\"id\"], name=course[\"name\"],\n                              details_url=course[\"details_url\"])\n    else:\n        selected = Course.get_selected()\n\n        # with Spinner.context(msg=\"Updated exercise metadata.\",\n        #                     waitmsg=\"Updating exercise metadata.\"):\n        print(\"Updating exercise data.\")\n        for exercise in api.get_exercises(selected):\n            old = None\n            try:\n                old = Exercise.byid(exercise[\"id\"])\n            except peewee.DoesNotExist:\n                old = None\n            if old is not None:\n                old.name = exercise[\"name\"]\n                old.course = selected.id\n                old.is_attempted = exercise[\"attempted\"]\n                old.is_completed = exercise[\"completed\"]\n                old.deadline = exercise.get(\"deadline\")\n                old.is_downloaded = os.path.isdir(old.path())\n                old.return_url = exercise[\"return_url\"]\n                old.zip_url = exercise[\"zip_url\"]\n                old.submissions_url = exercise[\"exercise_submissions_url\"]\n                old.save()\n                download_exercise(old, update=True)\n            else:\n                ex = Exercise.create(tid=exercise[\"id\"],\n                                     name=exercise[\"name\"],\n                                     course=selected.id,\n                                     is_attempted=exercise[\"attempted\"],\n                                     is_completed=exercise[\"completed\"],\n                                     deadline=exercise.get(\"deadline\"),\n                                     return_url=exercise[\"return_url\"],\n                                     zip_url=exercise[\"zip_url\"],\n                                     submissions_url=exercise[(\"exercise_\"\n                                                               \"submissions_\"\n                                                               \"url\")])\n                ex.is_downloaded = os.path.isdir(ex.path())\n                ex.save()", "27": "def get_conn(self):\n        \"\"\"\n        Returns a FTP connection object\n        \"\"\"\n        if self.conn is None:\n            params = self.get_connection(self.ftp_conn_id)\n            pasv = params.extra_dejson.get(\"passive\", True)\n            self.conn = ftplib.FTP(params.host, params.login, params.password)\n            self.conn.set_pasv(pasv)\n\n        return self.conn", "28": "def get_topic(self, topic_name):\n        '''\n        Retrieves the description for the specified topic.\n\n        topic_name:\n            Name of the topic.\n        '''\n        _validate_not_none('topic_name', topic_name)\n        request = HTTPRequest()\n        request.method = 'GET'\n        request.host = self._get_host()\n        request.path = '/' + _str(topic_name) + ''\n        request.path, request.query = self._httpclient._update_request_uri_query(request)  # pylint: disable=protected-access\n        request.headers = self._update_service_bus_header(request)\n        response = self._perform_request(request)\n\n        return _convert_response_to_topic(response)", "29": "def resolve_expression(self, *args, **kwargs):\n        \"\"\"Resolves expressions inside the dictionary.\"\"\"\n\n        result = dict()\n        for key, value in self.value.items():\n            if hasattr(value, 'resolve_expression'):\n                result[key] = value.resolve_expression(\n                    *args, **kwargs)\n            else:\n                result[key] = value\n\n        return HStoreValue(result)", "30": "def read_as_text(self):\n        '''Read and return the dataset contents as text.'''\n        return self.workspace._rest.read_intermediate_dataset_contents_text(\n            self.workspace.workspace_id,\n            self.experiment.experiment_id,\n            self.node_id,\n            self.port_name\n        )", "31": "def _submit(self, client, config, osutil, request_executor, io_executor,\n                transfer_future, bandwidth_limiter=None):\n        \"\"\"\n        :param client: The client associated with the transfer manager\n\n        :type config: s3transfer.manager.TransferConfig\n        :param config: The transfer config associated with the transfer\n            manager\n\n        :type osutil: s3transfer.utils.OSUtil\n        :param osutil: The os utility associated to the transfer manager\n\n        :type request_executor: s3transfer.futures.BoundedExecutor\n        :param request_executor: The request executor associated with the\n            transfer manager\n\n        :type io_executor: s3transfer.futures.BoundedExecutor\n        :param io_executor: The io executor associated with the\n            transfer manager\n\n        :type transfer_future: s3transfer.futures.TransferFuture\n        :param transfer_future: The transfer future associated with the\n            transfer request that tasks are being submitted for\n\n        :type bandwidth_limiter: s3transfer.bandwidth.BandwidthLimiter\n        :param bandwidth_limiter: The bandwidth limiter to use when\n            downloading streams\n        \"\"\"\n        if transfer_future.meta.size is None:\n            # If a size was not provided figure out the size for the\n            # user.\n            response = client.head_object(\n                Bucket=transfer_future.meta.call_args.bucket,\n                Key=transfer_future.meta.call_args.key,\n                **transfer_future.meta.call_args.extra_args\n            )\n            transfer_future.meta.provide_transfer_size(\n                response['ContentLength'])\n\n        download_output_manager = self._get_download_output_manager_cls(\n            transfer_future, osutil)(osutil, self._transfer_coordinator,\n                                     io_executor)\n\n        # If it is greater than threshold do a ranged download, otherwise\n        # do a regular GetObject download.\n        if transfer_future.meta.size < config.multipart_threshold:\n            self._submit_download_request(\n                client, config, osutil, request_executor, io_executor,\n                download_output_manager, transfer_future, bandwidth_limiter)\n        else:\n            self._submit_ranged_download_request(\n                client, config, osutil, request_executor, io_executor,\n                download_output_manager, transfer_future, bandwidth_limiter)", "32": "def delete(self, webhookId):\n        \"\"\"Delete a webhook, by ID.\n\n        Args:\n            webhookId(basestring): The ID of the webhook to be deleted.\n\n        Raises:\n            TypeError: If the parameter types are incorrect.\n            ApiError: If the Webex Teams cloud returns an error.\n\n        \"\"\"\n        check_type(webhookId, basestring, may_be_none=False)\n\n        # API request\n        self._session.delete(API_ENDPOINT + '/' + webhookId)", "33": "def draw(self):\n        \"\"\"\n        Redraws the menu and refreshes the screen. Should be called whenever something changes that needs to be redrawn.\n        \"\"\"\n\n        self.screen.border(0)\n        if self.title is not None:\n            self.screen.addstr(2, 2, self.title, curses.A_STANDOUT)\n        if self.subtitle is not None:\n            self.screen.addstr(4, 2, self.subtitle, curses.A_BOLD)\n\n        for index, item in enumerate(self.items):\n            if self.current_option == index:\n                text_style = self.highlight\n            else:\n                text_style = self.normal\n            self.screen.addstr(5 + index, 4, item.show(index), text_style)\n\n        screen_rows, screen_cols = CursesMenu.stdscr.getmaxyx()\n        top_row = 0\n        if 6 + len(self.items) > screen_rows:\n            if screen_rows + self.current_option < 6 + len(self.items):\n                top_row = self.current_option\n            else:\n                top_row = 6 + len(self.items) - screen_rows\n\n        self.screen.refresh(top_row, 0, 0, 0, screen_rows - 1, screen_cols - 1)", "34": "def convert_datetime_to_str(df, *, column: str, format: str, new_column: str = None):\n    \"\"\"\n    Convert datetime column into string column\n\n    ---\n\n    ### Parameters\n\n    *mandatory :*\n    - column (*str*): name of the column to format\n    - format (*str*): format of the result values (see [available formats](\n    https://docs.python.org/3/library/datetime.html#strftime-and-strptime-behavior))\n\n    *optional :*\n    - new_column (*str*): name of the output column. By default `column` is overwritten.\n    \"\"\"\n    new_column = new_column or column\n    df[new_column] = df[column].dt.strftime(format)\n    return df", "35": "def do_GEOHASHMEMBERS(self, geoh):\n        \"\"\"Return members of a geohash and its neighbors.\n        GEOHASHMEMBERS u09vej04 [NEIGHBORS 0]\"\"\"\n        geoh, with_neighbors = self._match_option('NEIGHBORS', geoh)\n        key = compute_geohash_key(geoh, with_neighbors != '0')\n        if key:\n            for id_ in DB.smembers(key):\n                r = Result(id_)\n                print('{} {}'.format(white(r), blue(r._id)))", "36": "def sll(sig, howMany) -> RtlSignalBase:\n    \"Logical shift left\"\n    width = sig._dtype.bit_length()\n    return sig[(width - howMany):]._concat(vec(0, howMany))", "37": "def plot_boundaries(all_boundaries, est_file, algo_ids=None, title=None,\n                    output_file=None):\n    \"\"\"Plots all the boundaries.\n\n    Parameters\n    ----------\n    all_boundaries: list\n        A list of np.arrays containing the times of the boundaries, one array\n        for each algorithm.\n    est_file: str\n        Path to the estimated file (JSON file)\n    algo_ids : list\n        List of algorithm ids to to read boundaries from.\n        If None, all algorithm ids are read.\n    title : str\n        Title of the plot. If None, the name of the file is printed instead.\n    \"\"\"\n    import matplotlib.pyplot as plt\n    N = len(all_boundaries)  # Number of lists of boundaries\n    if algo_ids is None:\n        algo_ids = io.get_algo_ids(est_file)\n\n    # Translate ids\n    for i, algo_id in enumerate(algo_ids):\n        algo_ids[i] = translate_ids[algo_id]\n    algo_ids = [\"GT\"] + algo_ids\n\n    figsize = (6, 4)\n    plt.figure(1, figsize=figsize, dpi=120, facecolor='w', edgecolor='k')\n    for i, boundaries in enumerate(all_boundaries):\n        color = \"b\"\n        if i == 0:\n            color = \"g\"\n        for b in boundaries:\n            plt.axvline(b, i / float(N), (i + 1) / float(N), color=color)\n        plt.axhline(i / float(N), color=\"k\", linewidth=1)\n\n    # Format plot\n    _plot_formatting(title, est_file, algo_ids, all_boundaries[0][-1], N,\n                     output_file)", "38": "def plot(\n            self, data, bbox=None, plot_type='scatter',\n            fig_kwargs=None, bmap_kwargs=None, plot_kwargs=None,\n            cbar_kwargs=None):\n        \"\"\"\n        Plot an array of data on a map using matplotlib and Basemap,\n        automatically matching the data to the Pandana network node positions.\n\n        Keyword arguments are passed to the plotting routine.\n\n        Parameters\n        ----------\n        data : pandas.Series\n            Numeric data with the same length and index as the nodes\n            in the network.\n        bbox : tuple, optional\n            (lat_min, lng_min, lat_max, lng_max)\n        plot_type : {'hexbin', 'scatter'}, optional\n        fig_kwargs : dict, optional\n            Keyword arguments that will be passed to\n            matplotlib.pyplot.subplots. Use this to specify things like\n            figure size or background color.\n        bmap_kwargs : dict, optional\n            Keyword arguments that will be passed to the Basemap constructor.\n            This can be used to specify a projection or coastline resolution.\n        plot_kwargs : dict, optional\n            Keyword arguments that will be passed to the matplotlib plotting\n            command used. Use this to control plot styles and color maps used.\n        cbar_kwargs : dict, optional\n            Keyword arguments passed to the Basemap.colorbar method.\n            Use this to control color bar location and label.\n\n        Returns\n        -------\n        bmap : Basemap\n        fig : matplotlib.Figure\n        ax : matplotlib.Axes\n\n        \"\"\"\n        from mpl_toolkits.basemap import Basemap\n\n        fig_kwargs = fig_kwargs or {}\n        bmap_kwargs = bmap_kwargs or {}\n        plot_kwargs = plot_kwargs or {}\n        cbar_kwargs = cbar_kwargs or {}\n\n        if not bbox:\n            bbox = (\n                self.nodes_df.y.min(),\n                self.nodes_df.x.min(),\n                self.nodes_df.y.max(),\n                self.nodes_df.x.max())\n\n        fig, ax = plt.subplots(**fig_kwargs)\n\n        bmap = Basemap(\n            bbox[1], bbox[0], bbox[3], bbox[2], ax=ax, **bmap_kwargs)\n        bmap.drawcoastlines()\n        bmap.drawmapboundary()\n\n        x, y = bmap(self.nodes_df.x.values, self.nodes_df.y.values)\n\n        if plot_type == 'scatter':\n            plot = bmap.scatter(\n                x, y, c=data.values, **plot_kwargs)\n        elif plot_type == 'hexbin':\n            plot = bmap.hexbin(\n                x, y, C=data.values, **plot_kwargs)\n\n        bmap.colorbar(plot, **cbar_kwargs)\n\n        return bmap, fig, ax", "39": "def isBin(self, type):\n        \"\"\"\n        is the type a byte array value?\n\n        :param type: PKCS#11 type like `CKA_MODULUS`\n        :rtype: bool\n        \"\"\"\n        return (not self.isBool(type)) \\\n            and (not self.isString(type)) \\\n            and (not self.isNum(type))", "40": "def sanger_variants(self, institute_id=None, case_id=None):\n        \"\"\"Return all variants with sanger information\n\n        Args:\n            institute_id(str)\n            case_id(str)\n\n        Returns:\n            res(pymongo.Cursor): A Cursor with all variants with sanger activity\n        \"\"\"\n        query = {'validation': {'$exists': True}}\n        if institute_id:\n            query['institute_id'] = institute_id\n        if case_id:\n            query['case_id'] = case_id\n\n        return self.variant_collection.find(query)", "41": "def create_submission(self, user_id, institute_id):\n        \"\"\"Create an open clinvar submission for a user and an institute\n           Args:\n                user_id(str): a user ID\n                institute_id(str): an institute ID\n\n           returns:\n                submission(obj): an open clinvar submission object\n        \"\"\"\n\n        submission_obj = {\n            'status' : 'open',\n            'created_at' : datetime.now(),\n            'user_id' : user_id,\n            'institute_id' : institute_id\n        }\n        LOG.info(\"Creating a new clinvar submission for user '%s' and institute %s\", user_id, institute_id)\n        result = self.clinvar_submission_collection.insert_one(submission_obj)\n        return result.inserted_id", "42": "def _all_get_or_create_table(self, where, tablename, description, expectedrows=None):\n        \"\"\"Creates a new table, or if the table already exists, returns it.\"\"\"\n        where_node = self._hdf5file.get_node(where)\n\n        if not tablename in where_node:\n            if not expectedrows is None:\n                table = self._hdf5file.create_table(where=where_node, name=tablename,\n                                              description=description, title=tablename,\n                                              expectedrows=expectedrows,\n                                              filters=self._all_get_filters())\n            else:\n                table = self._hdf5file.create_table(where=where_node, name=tablename,\n                                              description=description, title=tablename,\n                                              filters=self._all_get_filters())\n        else:\n            table = where_node._f_get_child(tablename)\n\n        return table", "43": "def set_many(self, new_values):\n        # type: (Iterable[B]) -> Callable[[S], T]\n        '''Set many foci to values taken by iterating over `new_values`.\n\n            >>> from lenses import lens\n            >>> lens.Each().set_many(range(4, 7))([0, 1, 2])\n            [4, 5, 6]\n        '''\n\n        def setter_many(state):\n            return self._optic.iterate(state, new_values)\n\n        return setter_many", "44": "def optimise_signal(self, analytes, min_points=5,\n                        threshold_mode='kde_first_max', \n                        threshold_mult=1., x_bias=0, filt=True,\n                        weights=None, mode='minimise',\n                        samples=None, subset=None):\n        \"\"\"\n        Optimise data selection based on specified analytes.\n\n        Identifies the longest possible contiguous data region in\n        the signal where the relative standard deviation (std) and \n        concentration of all analytes is minimised.\n\n        Optimisation is performed via a grid search of all possible\n        contiguous data regions. For each region, the mean std and\n        mean scaled analyte concentration ('amplitude') are calculated. \n        \n        The size and position of the optimal data region are identified \n        using threshold std and amplitude values. Thresholds are derived\n        from all calculated stds and amplitudes using the method specified\n        by `threshold_mode`. For example, using the 'kde_max' method, a\n        probability density function (PDF) is calculated for std and\n        amplitude values, and the threshold is set as the maximum of the\n        PDF. These thresholds are then used to identify the size and position\n        of the longest contiguous region where the std is below the threshold, \n        and the amplitude is either below the threshold.\n\n        All possible regions of the data that have at least\n        `min_points` are considered.\n\n        For a graphical demonstration of the action of signal_optimiser, \n        use `optimisation_plot`. \n\n        Parameters\n        ----------\n        d : latools.D object\n            An latools data object.\n        analytes : str or array-like\n            Which analytes to consider.\n        min_points : int\n            The minimum number of contiguous points to\n            consider.\n        threshold_mode : str\n            The method used to calculate the optimisation\n            thresholds. Can be 'mean', 'median', 'kde_max'\n            or 'bayes_mvs', or a custom function. If a\n            function, must take a 1D array, and return a\n            single, real number.\n        weights : array-like of length len(analytes)\n            An array of numbers specifying the importance of\n            each analyte considered. Larger number makes the\n            analyte have a greater effect on the optimisation.\n            Default is None.\n        \"\"\"\n        if samples is not None:\n            subset = self.make_subset(samples)\n        samples = self._get_samples(subset)\n\n        if isinstance(analytes, str):\n            analytes = [analytes]\n\n        self.minimal_analytes.update(analytes)\n\n        errs = []\n\n        with self.pbar.set(total=len(samples), desc='Optimising Data selection') as prog:\n            for s in samples:\n                e = self.data[s].signal_optimiser(analytes=analytes, min_points=min_points,\n                                                  threshold_mode=threshold_mode, threshold_mult=threshold_mult,\n                                                  x_bias=x_bias, weights=weights, filt=filt, mode=mode)\n                if e != '':\n                    errs.append(e)\n                prog.update()\n        \n        if len(errs) > 0:\n            print('\\nA Few Problems:\\n' + '\\n'.join(errs) + '\\n\\n  *** Check Optimisation Plots ***')", "45": "def tag_to_text(tag):\n    \"\"\"\n    :param tag: Beautiful soup tag\n    :return: Flattened text\n    \"\"\"\n    out = []\n    for item in tag.contents:\n        # If it has a name, it is a tag\n        if item.name:\n            out.append(tag_to_text(item))\n        else:\n            # Just text!\n            out.append(item)\n\n    return ' '.join(out)", "46": "def repository_exists(self, workspace, repo):\n        \"\"\"Return True if workspace contains repository name.\"\"\"\n        if not self.exists(workspace):\n            return False\n\n        workspaces = self.list()\n        return repo in workspaces[workspace][\"repositories\"]", "47": "def shutdown(self):\n        '''\n        send SIGTERM to the tagger child process\n        '''\n        if self._child:\n            try:\n                self._child.terminate()\n            except OSError, exc:\n                if exc.errno == 3:\n                    ## child is already gone, possibly because it ran\n                    ## out of memory and caused us to shutdown\n                    pass", "48": "def Geometry(*args, **kwargs):\n    \"\"\"Returns an ogr.Geometry instance optionally created from a geojson str\n    or dict. The spatial reference may also be provided.\n    \"\"\"\n    # Look for geojson as a positional or keyword arg.\n    arg = kwargs.pop('geojson', None) or len(args) and args[0]\n    try:\n        srs = kwargs.pop('srs', None) or arg.srs.wkt\n    except AttributeError:\n        srs = SpatialReference(4326)\n    if hasattr(arg, 'keys'):\n        geom = ogr.CreateGeometryFromJson(json.dumps(arg))\n    elif hasattr(arg, 'startswith'):\n        # WKB as hexadecimal string.\n        char = arg[0] if arg else ' '\n        i = char if isinstance(char, int) else ord(char)\n        if i in (0, 1):\n            geom = ogr.CreateGeometryFromWkb(arg)\n        elif arg.startswith('{'):\n            geom = ogr.CreateGeometryFromJson(arg)\n        elif arg.startswith('<gml'):\n            geom = ogr.CreateGeometryFromGML(arg)\n        else:\n            raise ValueError('Invalid geometry value: %s' % arg)\n    elif hasattr(arg, 'wkb'):\n        geom = ogr.CreateGeometryFromWkb(bytes(arg.wkb))\n    else:\n        geom = ogr.Geometry(*args, **kwargs)\n    if geom:\n        if not isinstance(srs, SpatialReference):\n            srs = SpatialReference(srs)\n        geom.AssignSpatialReference(srs)\n    return geom", "49": "def argument_parser(args):\n    \"\"\"Argparse logic, command line options.\n\n    Args:\n        args: sys.argv[1:], everything passed to the program after its name\n\n    Returns:\n        A tuple of:\n            a list of words/letters to search\n            a boolean to declare if we want to use the sowpods words file\n            a boolean to declare if we want to output anagrams by length\n            a string of starting characters to find anagrams based on\n            a string of ending characters to find anagrams based on\n\n    Raises:\n        SystemExit if the user passes invalid arguments, --version or --help\n    \"\"\"\n\n    parser = argparse.ArgumentParser(\n        prog=\"nagaram\",\n        description=\"Finds Scabble anagrams.\",\n        formatter_class=argparse.RawDescriptionHelpFormatter,\n        add_help=False,\n    )\n\n    parser.add_argument(\n        \"-h\", \"--help\",\n        dest=\"help\",\n        action=\"store_true\",\n        default=False,\n    )\n\n    parser.add_argument(\n        \"--sowpods\",\n        dest=\"sowpods\",\n        action=\"store_true\",\n        default=False,\n    )\n\n    parser.add_argument(\n        \"--length\",\n        \"-l\",\n        dest=\"length\",\n        action=\"store_true\",\n        default=False,\n    )\n\n    parser.add_argument(\n        \"--starts-with\",\n        \"-s\",\n        dest=\"starts_with\",\n        metavar=\"chars\",\n        default=\"\",\n        nargs=1,\n        type=str,\n    )\n\n    parser.add_argument(\n        \"--ends-with\",\n        \"-e\",\n        dest=\"ends_with\",\n        metavar=\"chars\",\n        default=\"\",\n        nargs=1,\n        type=str,\n    )\n\n    parser.add_argument(\n        \"--version\",\n        \"-v\",\n        action=\"version\",\n        version=\"Nagaram {0} (Released: {1})\".format(\n            nagaram.__version__,\n            nagaram.__release_date__,\n        )\n    )\n\n    parser.add_argument(\n        dest=\"wordlist\",\n        metavar=\"letters to find anagrams with (? for anything, _ for blanks)\",\n        nargs=argparse.REMAINDER,\n    )\n\n    settings = parser.parse_args(args)\n\n    if settings.help:\n        raise SystemExit(nagaram.__doc__.strip())\n\n    if not settings.wordlist:\n        raise SystemExit(parser.print_usage())\n\n    if settings.starts_with:\n        settings.starts_with = settings.starts_with[0]\n    if settings.ends_with:\n        settings.ends_with = settings.ends_with[0]\n\n    return (settings.wordlist, settings.sowpods, settings.length,\n            settings.starts_with, settings.ends_with)"}